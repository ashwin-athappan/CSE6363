{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!pip install lightning\n",
    "!pip install pytorch-lightning\n",
    "!pip install matplotlib\n",
    "!pip install torchvision"
   ],
   "id": "1d27e991217bf9aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T21:13:10.424219Z",
     "start_time": "2025-04-08T21:13:10.421853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ssl\n",
    "import torch\n",
    "import torchmetrics\n",
    "from torch import nn\n",
    "import lightning as L\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import Imagenette\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping"
   ],
   "id": "19cb973410340e30",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Basic CNN",
   "id": "d08863ec647b809c"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-08T21:13:43.603935Z",
     "start_time": "2025-04-08T21:13:43.599073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BasicCNN(L.LightningModule):\n",
    "    def __init__(self, num_classes=10, size=8, in_channels=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolutional Layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(128 * size * size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.train_losses.append(loss.item())\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "\n",
    "        self.accuracy(y_hat, y)\n",
    "        self.val_losses.append(loss.item())\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.log(\"val_accuracy\", self.accuracy)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "\n",
    "        self.accuracy(y_hat, y)\n",
    "\n",
    "        self.log(\"test_loss\", loss)\n",
    "        self.log(\"test_accuracy\", self.accuracy)\n",
    "\n",
    "    def train_epoch(self):\n",
    "        avg_train_loss = sum(self.train_losses)/len(self.train_losses) if self.train_losses else None\n",
    "        print(f'Epoch {self.current_epoch}: Average Training Loss:{avg_train_loss}')\n",
    "\n",
    "    def validation_epoch(self):\n",
    "        avg_val_loss = sum(self.val_losses)/len(self.val_losses) if self.val_losses else None\n",
    "        print(f'Epoch {self.current_epoch}: Average Validation Loss:{avg_val_loss}')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ResNet CNN",
   "id": "9dc424bba933568f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T21:13:44.570895Z",
     "start_time": "2025-04-08T21:13:44.565410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ResNet18(L.LightningModule):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet18, self).__init__()\n",
    "\n",
    "        # Initial convolutional layer\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Convolutional layers without residual connections\n",
    "        self.layer1 = self._make_layer(64, 64, stride=1)\n",
    "        self.layer2 = self._make_layer(64, 128, stride=2)\n",
    "        self.layer3 = self._make_layer(128, 256, stride=2)\n",
    "        self.layer4 = self._make_layer(256, 512, stride=2)\n",
    "\n",
    "        # Fully connected layer for classification\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def _make_layer(self, in_channels, out_channels, stride):\n",
    "        layers = [\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.train_losses.append(loss.item())\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.accuracy(y_hat, y)\n",
    "        self.val_losses.append(loss.item())\n",
    "        self.log(\"val_accuracy\", self.accuracy, prog_bar=True)\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.accuracy(y_hat, y)\n",
    "        self.log(\"test_accuracy\", self.accuracy, prog_bar=True)\n",
    "        self.log(\"test_loss\", loss)\n",
    "\n",
    "    def train_epoch(self):\n",
    "        avg_train_loss = sum(self.train_losses)/len(self.train_losses) if self.train_losses else None\n",
    "        print(f'Epoch {self.current_epoch}: Average Training Loss:{avg_train_loss}')\n",
    "\n",
    "    def validation_epoch(self):\n",
    "        avg_val_loss = sum(self.val_losses)/len(self.val_losses) if self.val_losses else None\n",
    "        print(f'Epoch {self.current_epoch}: Average Validation Loss:{avg_val_loss}')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ],
   "id": "7810731d0be57a62",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Regularization CNN",
   "id": "61612661fa09bec8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T21:13:46.092440Z",
     "start_time": "2025-04-08T21:13:46.087294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RegularizationCNN(L.LightningModule):\n",
    "    def __init__(self, num_classes=10, size=8):\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolutional Layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(128 * size * size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),  # Dropout for regularization\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.train_losses.append(loss.item())\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "\n",
    "        self.accuracy(y_hat, y)\n",
    "        self.val_losses.append(loss.item())\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.log(\"val_accuracy\", self.accuracy)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "\n",
    "        self.accuracy(y_hat, y)\n",
    "\n",
    "        self.log(\"test_loss\", loss)\n",
    "        self.log(\"test_accuracy\", self.accuracy)\n",
    "\n",
    "    def train_epoch(self):\n",
    "        avg_train_loss = sum(self.train_losses)/len(self.train_losses) if self.train_losses else None\n",
    "        print(f'Epoch {self.current_epoch}: Average Training Loss:{avg_train_loss}')\n",
    "\n",
    "    def validation_epoch(self):\n",
    "        avg_val_loss = sum(self.val_losses)/len(self.val_losses) if self.val_losses else None\n",
    "        print(f'Epoch {self.current_epoch}: Average Validation Loss:{avg_val_loss}')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ],
   "id": "6264d58ab3edf79b",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prepare the dataset",
   "id": "db669c45f3f69146"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T21:13:48.430874Z",
     "start_time": "2025-04-08T21:13:48.409799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare the dataset\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.CenterCrop(160),\n",
    "    transforms.Resize(64),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "    transforms.Grayscale()\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.CenterCrop(160),\n",
    "    transforms.Resize(64),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "    transforms.Grayscale()\n",
    "])\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "train_dataset = Imagenette(\"data/imagenette/train/\", split=\"train\", size=\"160px\", download=True, transform=train_transforms)\n",
    "\n",
    "train_set_size = int(len(train_dataset) * 0.9)\n",
    "val_set_size = len(train_dataset) - train_set_size\n",
    "\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_set_size, val_set_size], generator=seed)\n",
    "val_dataset.dataset.transform = test_transforms\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, num_workers=8, shuffle=True, persistent_workers=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=128, num_workers=8, shuffle=False, persistent_workers=True)\n",
    "\n",
    "test_dataset = Imagenette(\"data/imagenette/test/\", split=\"val\", size=\"160px\", download=True, transform=test_transforms)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=256, num_workers=8, shuffle=False)"
   ],
   "id": "853e1cbef1fbce8e",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Basic CNN Model",
   "id": "20367ef31c429512"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T21:13:50.289583Z",
     "start_time": "2025-04-08T21:13:50.281783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "basic_cnn_model = BasicCNN()\n",
    "\n",
    "basic_cnn_early_stop_callback = EarlyStopping(monitor=\"val_loss\",\n",
    "                                    mode=\"min\",\n",
    "                                    patience=5)\n",
    "\n",
    "basic_cnn_checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    dirpath=\"./checkpoints\",\n",
    "    filename=\"model_weights\",\n",
    "    save_top_k=1,\n",
    "    verbose=True\n",
    ")"
   ],
   "id": "49566bfb54f592c1",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train the Basic CNN",
   "id": "4811873fc6bb678a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T21:15:39.546002Z",
     "start_time": "2025-04-08T21:14:04.589066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "basic_cnn_trainer = L.Trainer(callbacks=[basic_cnn_early_stop_callback, basic_cnn_checkpoint_callback], max_epochs=10)\n",
    "trainer.fit(model=basic_cnn_model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ],
   "id": "28415c99964c80ea",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name        | Type               | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | conv_layers | Sequential         | 92.7 K | train\n",
      "1 | fc_layers   | Sequential         | 2.1 M  | train\n",
      "2 | accuracy    | MulticlassAccuracy | 0      | train\n",
      "-----------------------------------------------------------\n",
      "2.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.2 M     Total params\n",
      "8.771     Total estimated model params size (MB)\n",
      "15        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 67/67 [00:01<00:00, 37.79it/s, v_num=9]          \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:00, 149.43it/s]\u001B[A\n",
      "Validation DataLoader 0:  25%|██▌       | 2/8 [00:00<00:00, 130.03it/s]\u001B[A\n",
      "Validation DataLoader 0:  38%|███▊      | 3/8 [00:00<00:00, 136.05it/s]\u001B[A\n",
      "Validation DataLoader 0:  50%|█████     | 4/8 [00:00<00:00, 139.17it/s]\u001B[A\n",
      "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:00<00:00, 138.85it/s]\u001B[A\n",
      "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:00<00:00, 137.25it/s]\u001B[A\n",
      "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:00<00:00, 136.19it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 8/8 [00:00<00:00, 115.77it/s]\u001B[A\n",
      "Epoch 9: 100%|██████████| 67/67 [00:01<00:00, 34.45it/s, v_num=9]      \u001B[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 670: 'val_loss' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 67/67 [00:01<00:00, 34.37it/s, v_num=9]\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test the Basic CNN",
   "id": "2c4830ae1eded832"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "basic_cnn_trainer.test(model=basic_cnn_model, dataloaders=test_loader)",
   "id": "c1b8cfd160fe20dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Plot training and validation losses",
   "id": "7f617acb145ed653"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot training and validation losses\n",
    "def plot_losses(model):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(model.train_losses, label=\"Training Loss\")\n",
    "    plt.plot(model.val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training and Validation Loss over Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_losses(basic_cnn_model)"
   ],
   "id": "32e565dcc57c08d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ResNet 18 Model",
   "id": "c51e3ef4b4ff3b4a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T21:13:57.957965Z",
     "start_time": "2025-04-08T21:13:57.945117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "resnet_18_model = ResNet18()\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\",\n",
    "                                    mode=\"min\",\n",
    "                                    patience=5)\n",
    "\n",
    "resnet_18_checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\"\n",
    ")"
   ],
   "id": "f99c0882b4242aa7",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "30c6a6b5bc457e76"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
